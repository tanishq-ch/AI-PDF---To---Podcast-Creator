# import time
# import os

# def generate_audio_from_script(script_text, output_path):
#     """
#     [PLACEHOLDER] Converts a formatted script into an MP3 audio file using a TTS model.
    
#     This function simulates the audio generation process. In a full implementation,
#     this is where you would integrate the Dia TTS model.

#     Args:
#         script_text (str): The script generated by the language model.
#         output_path (str): The full path where the MP3 file should be saved.

#     Returns:
#         bool: True if successful, False otherwise.
#     """
#     print("--- SIMULATING AUDIO GENERATION ---")
#     print(f"Script to be voiced:\n{script_text[:200]}...") # Print first 200 chars
#     print(f"Output path: {output_path}")

#     try:
#         # Simulate a time-consuming AI process
#         time.sleep(5) 
        
#         # Create a dummy file to represent the generated podcast
#         with open(output_path, 'w') as f:
#             f.write("This is a dummy MP3 file created by the placeholder audio generator.")
        
#         print("--- SIMULATION COMPLETE ---")
#         return True
#     except Exception as e:
#         print(f"Error during audio generation simulation: {e}")
#         return False

# # --- EXAMPLE OF REAL DIA INTEGRATION (FOR FUTURE IMPLEMENTATION) ---
# #
# # from dia.instrument import instrument
# #
# # def real_generate_audio_from_script(script_text, output_path):
# #     try:
# #         # NOTE: This is a conceptual example. You would need to follow the
# #         # official Dia documentation for correct usage, model loading, etc.
# #         # This might involve loading models into memory beforehand.
# #         
# #         # Dia can often take scripts with speaker tags and non-verbal cues
# #         # e.g., script = "[S1] Hello there! [S2] Hi! (laughs)"
# #
# #         wav_data = instrument(script_text)
# #
# #         # Save the wav_data to the output_path as an mp3
# #         # You might need a library like pydub to convert WAV to MP3
# #         # from pydub import AudioSegment
# #         # audio = AudioSegment(data=wav_data, sample_width=2, frame_rate=24000, channels=1)
# #         # audio.export(output_path, format="mp3")
# #
# #         return True
# #     except Exception as e:
# #         print(f"Error generating audio with Dia: {e}")
# #         return False


# import os
# import torch
# from TTS.api import TTS
# from pydub import AudioSegment

# # --- Model Loading (Singleton Pattern) ---
# # This ensures the large TTS model is loaded into memory only ONCE.
# tts_model = None

# def get_tts_model():
#     """Initializes and returns the TTS model, loading it if not already in memory."""
#     global tts_model
#     if tts_model is None:
#         print("--- LOADING TTS MODEL INTO MEMORY (this may take a moment) ---")
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         print(f"--- TTS will use device: {device} ---")
#         try:
#             # --- NEW: Manually approve the trusted class for PyTorch's secure loader ---
#             # This is the direct code-based fix for the 'weights_only' error.
#             # We are still using the powerful XTTS-v2 model
#             tts_model = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
#             print("--- TTS MODEL LOADED SUCCESSFULLY ---")
#         except Exception as e:
#             print(f"!!! FAILED TO LOAD TTS MODEL: {e} !!!")
#             raise e
#     return tts_model

# def generate_audio_from_script(script_text, output_path):
#     """
#     Converts a formatted script into a multi-speaker MP3 audio file using
#     Coqui XTTS-v2's high-quality built-in voices.

#     Args:
#         script_text (str): The script from the LLM, with "Host:" and "Expert:" tags.
#         output_path (str): The full path for the final MP3 file.

#     Returns:
#         bool: True if successful, False otherwise.
#     """
#     print(f"--- INITIATING BUILT-IN VOICE AUDIO GENERATION for {os.path.basename(output_path)} ---")
    
#     try:
#         model = get_tts_model()
        
#         # --- Define the pre-built speaker voices to use ---
#         # These are high-quality voices included with the XTTS-v2 model.
#         # You can experiment with other speakers by printing model.speakers
#         HOST_SPEAKER_ID = "Claribel Dervla"
#         EXPERT_SPEAKER_ID = "Tammie Ema"

#         # Create a temporary directory for individual audio clips
#         temp_dir = "temp_audio_clips"
#         os.makedirs(temp_dir, exist_ok=True)

#         lines = script_text.strip().split('\n')
#         generated_clips = []

#         # Process each line of the script
#         for i, line in enumerate(lines):
#             line = line.strip()
#             if not line:
#                 continue

#             speaker_id = None
#             text_to_speak = ""

#             if line.lower().startswith("host:"):
#                 speaker_id = HOST_SPEAKER_ID
#                 text_to_speak = line[5:].strip()
#             elif line.lower().startswith("expert:"):
#                 speaker_id = EXPERT_SPEAKER_ID
#                 text_to_speak = line[7:].strip()
#             else:
#                 # Skip any lines that don't conform to the format
#                 continue

#             if not text_to_speak:
#                 continue

#             # Generate the audio clip for the current line using the built-in speaker ID
#             clip_path = os.path.join(temp_dir, f"clip_{i}.wav")
#             print(f"Generating clip {i} for speaker '{speaker_id}'...")
            
#             # The key change is here: we use the 'speaker' argument, not 'speaker_wav'
#             model.tts_to_file(
#                 text=text_to_speak,
#                 speaker=speaker_id,
#                 language="en",
#                 file_path=clip_path
#             )
#             generated_clips.append(clip_path)

#         # Stitch the generated clips together using pydub
#         print("--- Stitching audio clips together... ---")
#         final_podcast = AudioSegment.silent(duration=500) # Start with a brief pause

#         for clip_path in generated_clips:
#             segment = AudioSegment.from_wav(clip_path)
#             final_podcast += segment
#             final_podcast += AudioSegment.silent(duration=750) # Pause between speakers
        
#         # Export the final combined audio to the desired output path as an MP3
#         print(f"--- Exporting final MP3 to {output_path} ---")
#         final_podcast.export(output_path, format="mp3")

#         # --- Cleanup ---
#         print("--- Cleaning up temporary files... ---")
#         for clip_path in generated_clips:
#             os.remove(clip_path)
#         os.rmdir(temp_dir)
        
#         print(f"--- AUDIO GENERATION SUCCESSFUL for {os.path.basename(output_path)} ---")
#         return True

#     except Exception as e:
#         print(f"!!! AUDIO GENERATION FAILED: {e} !!!")
#         # Ensure cleanup happens even on failure
#         if 'temp_dir' in locals() and os.path.exists(temp_dir):
#             for f in os.listdir(temp_dir):
#                 os.remove(os.path.join(temp_dir, f))
#             os.rmdir(temp_dir)
#         return False


import os
import torch
from pydub import AudioSegment
from TTS.api import TTS

# --- Model Loading (Singleton Pattern) ---
# A clean, simple version without any fixes, because we've fixed the environment.
tts_model = None

def get_tts_model():
    """Initializes and returns the TTS model, loading it if not already in memory."""
    global tts_model
    if tts_model is None:
        print("--- LOADING TTS MODEL INTO MEMORY (this may take a moment) ---")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"--- TTS will use device: {device} ---")
        try:
            tts_model = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
            print("--- TTS MODEL LOADED SUCCESSFULLY ---")
        except Exception as e:
            print(f"!!! FAILED TO LOAD TTS MODEL: {e} !!!")
            raise e
    return tts_model

def generate_audio_from_script(script_text, output_path):
    """
    Converts a formatted script into a multi-speaker MP3 audio file using
    Coqui XTTS-v2's high-quality built-in voices.
    """
    print(f"--- INITIATING BUILT-IN VOICE AUDIO GENERATION for {os.path.basename(output_path)} ---")
    
    try:
        model = get_tts_model()
        
        HOST_SPEAKER_ID = "Claribel Dervla"
        EXPERT_SPEAKER_ID = "Tammie Ema"

        temp_dir = "temp_audio_clips"
        os.makedirs(temp_dir, exist_ok=True)

        lines = script_text.strip().split('\n')
        generated_clips = []

        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            speaker_id = None
            text_to_speak = ""

            if line.lower().startswith("host:"):
                speaker_id = HOST_SPEAKER_ID
                text_to_speak = line[5:].strip()
            elif line.lower().startswith("expert:"):
                speaker_id = EXPERT_SPEAKER_ID
                text_to_speak = line[7:].strip()
            else:
                continue

            if not text_to_speak:
                continue
            
            clip_path = os.path.join(temp_dir, f"clip_{i}.wav")
            print(f"Generating clip {i} for speaker '{speaker_id}'...")
            
            model.tts_to_file(
                text=text_to_speak,
                speaker=speaker_id,
                language="en",
                file_path=clip_path
            )
            generated_clips.append(clip_path)

        print("--- Stitching audio clips together... ---")
        final_podcast = AudioSegment.silent(duration=500)

        for clip_path in generated_clips:
            segment = AudioSegment.from_wav(clip_path)
            final_podcast += segment
            final_podcast += AudioSegment.silent(duration=750)
        
        print(f"--- Exporting final MP3 to {output_path} ---")
        final_podcast.export(output_path, format="mp3")

        print("--- Cleaning up temporary files... ---")
        for clip_path in generated_clips:
            os.remove(clip_path)
        os.rmdir(temp_dir)
        
        print(f"--- AUDIO GENERATION SUCCESSFUL for {os.path.basename(output_path)} ---")
        return True

    except Exception as e:
        print(f"!!! AUDIO GENERATION FAILED: {e} !!!")
        if 'temp_dir' in locals() and os.path.exists(temp_dir):
            for f in os.listdir(temp_dir):
                os.remove(os.path.join(temp_dir, f))
            os.rmdir(temp_dir)
        return False